{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miditok import REMI, TokenizerConfig\n",
    "from miditoolkit import MidiFile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import numpy as np\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "model_list = os.listdir(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN100k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 176.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN10k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN110k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 176.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN120k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN130k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 178.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN140k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:07<00:00, 137.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN150k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 177.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN160k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN170k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 177.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN180k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN20k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN30k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN40k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN50k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN60k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN70k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN80k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:07<00:00, 139.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN90k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 178.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_GAN10k.pt\n",
      "number of parameters: 85.15M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:05<00:00, 179.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for model_name in model_list:\n",
    "    if \"GAN\" not in model_name:\n",
    "        continue\n",
    "    print(model_name)\n",
    "\n",
    "    init_from = 'resume'\n",
    "    out_dir = 'out'\n",
    "    use_model = model_name\n",
    "    start = \"\\n\"\n",
    "    num_samples = 1\n",
    "    max_new_tokens = 1024\n",
    "    temperature = 0.8\n",
    "    top_k = 200\n",
    "    seed = 1337\n",
    "    device = 'cuda'\n",
    "    dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    compile = False\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "    ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, use_model)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    REMIconfig = TokenizerConfig(nb_velocities=16, use_chords=False, use_programs=True)\n",
    "    tokenizer = REMI(REMIconfig)\n",
    "\n",
    "    midis_path = \"test/IN\"\n",
    "    midi_name = \"crt.mid\"\n",
    "    midi = MidiFile(os.path.join(midis_path, midi_name))\n",
    "    tokens = tokenizer(midi).ids\n",
    "    tokens = torch.tensor(tokens, requires_grad=False, device=device)[None, ...]\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                y = model.generate(tokens, max_new_tokens, temperature=temperature, top_k=top_k).squeeze().cpu().numpy()\n",
    "    \n",
    "    converted_back_midi = tokenizer(y)\n",
    "    converted_back_midi.dump(f\"test/OUT/CRT/{model_name.split('.')[0]}_{midi_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
